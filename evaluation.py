import re
from openai import OpenAI
import os
import csv
import json

# Set up Gpt
os.environ["OPENAI_API_KEY"] = ""
client = OpenAI(
    api_key=os.environ['OPENAI_API_KEY'],  # this is also the default, it can be omitted
)

def api_function(prompt):
    """
       Interact with the OpenAI API to get a response based on the provided prompt.

       Args:
           prompt (list): A list of dictionaries representing the conversation history
                          and the current user input. Each dictionary contains a 'role'
                          and 'content' key.

       Returns:
           str: The generated response text from the OpenAI API, or an error message
                if an exception occurs.
       """
    try:
        response = client.chat.completions.create(
            model='gpt-4o',
            messages=prompt,
            temperature=0.7,
            max_tokens=100,
        )
        generated_text = response.choices[0].message.content if hasattr(response.choices[0].message, 'content') else ''
    except Exception as e:
        return f"An error occurred: {str(e)}"
    return generated_text

def evaluate_response(response, context):
    """
        Evaluate the chatbot's response based on accuracy, relevance, and user satisfaction.

        Args:
            response (str): The response generated by the assistant.
            context (str): The chat context containing the conversation history.

        Returns:
            dict: The evaluation scores in JSON format.
        """
    prompt = [
        {"role": "system", "content": "You are an evaluator for an e-commerce chatbot. Evaluate the assistant's response based on the following metrics:\
                accuracy, relevance, and user satisfaction. Use a scale of 1-5 for each metric. Definitions are as follows:\n\
                Accuracy: Does the response appear as you would expect from a perfect chatbot?\n\
                Relevance: How well the response addresses the user's query or context.\n\
                User satisfaction: How likely the user is satisfied with the response.\
                You will be given the context and the chatbot response. Please return the evaluation scores in the following JSON format:\
                {'accuracy': score, 'relevance': score, 'user satisfaction': score}"},
        {"role": "user", "content": "Context: Bot: Welcome to BestCompany! I'm Anne, your virtual assistant. This is an open-language chat, but I can also handle some requests automatically. Common actions I can assist with include checking order status, providing return policy information, and connecting you with a human representative. How can I help you today?\n\
                             User: I need help with tracking my package.\n\
                             Bot: I understand that you want help with Check Order Status. If I'm correct, please type 'yes'. Otherwise, type 'no'.\n\
                             User: yes\n\
                        Chatbot Response: The status of your order 1355D is: In Transit. If you have any other questions, feel free to ask!"},
        {"role": "assistant", "content": "{'accuracy': 5, 'relevance': 5, 'user satisfaction': 5}"},
        {"role": "user", "content": f"Context: {context}\nChatbot Response: {response}\n\n "}
    ]
    generated_text = api_function(prompt)
    try:
        # Replace single quotes with double quotes to fix JSON format
        generated_text = generated_text.replace("'", "\"")
        scores = json.loads(generated_text)
    except json.JSONDecodeError as e:
        print(f"JSON Decode Error: {e}")
        print(f"Generated Text: {generated_text}")
        scores = {"accuracy": 3, "relevance": 3, "user satisfaction": 3}
    return scores

def anotate_data():
    """
    Evaluate predefined dialogues by passing them through an LLM annotator to assess capability.

    Load dialogues from a CSV file, split each dialogue, and evaluate each response using the context up to that point.
    The evaluation results are then stored in a JSON file.
    """

    # Load predefined dialogues from a CSV file
    predefined_dialogues = []
    with open('predefined_dialogues.csv', mode='r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        for row in reader:
            predefined_dialogues.append({
                "dialogue_id": row['Dialogue ID'],
                "context": row['Dialogue']
            })

    evaluation_results = []
    # Evaluate each dialogue
    for dialogue in predefined_dialogues:
        dialogue_id = dialogue["dialogue_id"]
        # Use regular expressions to split the context into parts
        context_parts = re.split(r'(Bot:|User:)', dialogue["context"])
        context = ""

        # We want to evaluate each response of the chatbot. Process the context parts
        for i in range(len(context_parts)):
            if context_parts[i] in ["Bot:", "User:"]:
                if i + 1 < len(context_parts):
                    context += context_parts[i] + context_parts[i + 1].strip() + " "
            if i + 2 < len(context_parts) and context_parts[i + 2] == "Bot:":
                next_bot_response = context_parts[i + 3].strip()
                scores = evaluate_response(next_bot_response, context.strip())
                evaluation_results.append({
                    "dialogue_id": dialogue_id,
                    "context": context.strip(),
                    "response": next_bot_response,
                    "accuracy": scores['accuracy'],
                    "relevance": scores['relevance'],
                    "user_satisfaction": scores['user satisfaction']
                })

    with open('evaluation_results.json', 'w') as f:
        json.dump(evaluation_results, f, indent=4)

    print("Evaluation complete. Results saved to evaluation_results.json.")

def calculate_mean_scores(json_data):
    """
    Calculate the mean scores for accuracy, relevance, and user satisfaction from the evaluation results.

    Args:
        json_data (list): The evaluation results loaded from a JSON file.

    Returns:
        dict: The mean scores for accuracy, relevance, and user satisfaction.
    """
    total_scores = {"accuracy": 0, "relevance": 0, "user satisfaction": 0}
    count = 0

    for entry in json_data:
        accuracy = entry.get("accuracy", 0)
        relevance = entry.get("relevance", 0)
        user_satisfaction = entry.get("user_satisfaction", 0)
        total_scores["accuracy"] += accuracy
        total_scores["relevance"] += relevance
        total_scores["user satisfaction"] += user_satisfaction
        count += 1

    if count == 0:
        return {"accuracy": 0, "relevance": 0, "user satisfaction": 0}

    mean_scores = {
        "accuracy": total_scores["accuracy"] / count,
        "relevance": total_scores["relevance"] / count,
        "user satisfaction": total_scores["user satisfaction"] / count
    }
    return mean_scores

if __name__ == '__main__':
    # Annotate the data and save the results to a JSON file. Already done
    anotate_data()

    # Load the JSON data
    json_file_path = 'evaluation_results.json'
    with open(json_file_path, 'r', encoding='utf-8') as file:
        json_data = json.load(file)

    # Calculate and print mean scores
    mean_scores = calculate_mean_scores(json_data)
    print("Mean Scores:")
    print(f"Accuracy: {mean_scores['accuracy']:.2f}")
    print(f"Relevance: {mean_scores['relevance']:.2f}")
    print(f"User Satisfaction: {mean_scores['user satisfaction']:.2f}")
    print(f"Eval dataset size: {len(json_data)}")